# -*- coding: utf-8 -*-
"""LLMLingua.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q5oKRGLeYPkMaXgbZdh0HHDQCSE3gdVf
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install openai

!pip install llmlingua

pip install huggingface

pip install datasets

import numpy as np
from tqdm import tqdm
from datasets import load_dataset

gsm8k = load_dataset('gsm8k', 'main')

validation_index = np.load('/content/drive/MyDrive/업무 관련 참고자료/AI advanced/data/validation_index.npy')
validation_data = gsm8k['train'].select(validation_index)
gsm8k_test = gsm8k['test']

prompt_complex = open('/content/drive/MyDrive/업무 관련 참고자료/AI advanced/data/prompt_hardest.txt').read()

gsm8k



import os
from openai import OpenAI
from tenacity import (
    retry,
    stop_after_attempt,
    wait_chain,
    wait_fixed
)

os.environ["OPENAI_API_KEY"] = ""
client = OpenAI()

@retry(wait=wait_chain(*[wait_fixed(3) for i in range(3)] +
                        [wait_fixed(5) for i in range(2)] +
                        [wait_fixed(10)]))
def get_completion(prompt, model_name='gpt-3.5-turbo'):
  completion = client.chat.completions.create(
  model=model_name,
  messages=[
          {"role": "system", "content": "너는 질문에 적절한 답변을 하는 어시스턴트야"},
          {"role": "user", "content": prompt},
    ]
  )
  return completion



import re

def test_answer(pred_str, ans_str):
  pattern = '\d\.?\d+'
  pred = re.findall(pattern, pred_str)
  if(len(pred)>=1):
    pred = pred[-1]
    gold = re.findall(pattern, ans_str)
    gold = gold[-1]
    return pred == gold

  else:
    return False

def parse_pred_ans(filename):
    with open(filename) as fd: lines = fd.readlines()
    am, a = None, None
    num_q, acc = 0, 0
    current_mode = 'none'
    questions = []
    ans_pred = []
    ans_gold = []
    for l in lines:
        if(l.startswith('Q: ')):
            if(am is not None and a is not None):
                questions.append(q)
                ans_pred.append(am)
                ans_gold.append(a)
                if(test_answer(am, a)):
                    acc += 1
            current_mode = 'q'
            q = l
            num_q += 1
        elif(l.startswith('A_model:')):
            current_mode = 'am'
            am = l
        elif(l.startswith('A:')):
            current_mode = 'a'
            a = l
        else:
            if(current_mode == 'q'): q += l
            elif(current_mode == 'am'): am += l
            elif(current_mode == 'a'): a += l
            else:
                raise ValueError(current_mode)

    questions.append(q)
    ans_pred.append(am)
    ans_gold.append(a)
    if(test_answer(am, a)):
        acc += 1
    print('num_q %d correct %d ratio %.4f' % (num_q, acc, float(acc / num_q)))
    return questions, ans_pred, ans_gold

def test_finished(ans_model):
    if('answer is' in ans_model): return True
    else: return False

def extract_ans(ans_model):
    ans_model = ans_model.split('\n')
    ans = []
    residual = []
    for li, al in enumerate(ans_model):
        ans.append(al)
        if('answer is' in al):
            break
    residual = list(ans_model[li + 1:])
    ans = '\n'.join(ans)
    residual = '\n'.join(residual)
    return ans, residual

prompt_q = prompt_complex + '\nQuestion: ' + gsm8k_test[1]['question'] + '\n'

print(prompt_q)

prompt_q

completion

prompt_q

!pip install optimum

pip install auto-gptq

from llmlingua import PromptCompressor

llm_lingua = PromptCompressor("microsoft/phi-2")

from llmlingua import PromptCompressor

llm_lingua = PromptCompressor("beomi/Llama-3-KoEn-8B")

prompt = """ """ # 사내 문서를 넣어서 테스트해서 지웠습니다.

compressed_prompt = llm_lingua.compress_prompt(prompt,
                                               instruction="",
                                               question=question,
                                               target_token=200)

print(compressed_prompt)



compressed_prompt = llm_lingua.compress_prompt(prompt,
                                               instruction="",
                                               question=question,
                                               target_token=300)

print(compressed_prompt['compressed_prompt'])

completion = get_completion(prompt=prompt+f" # 질문: {question}")

completion

get_completion(prompt=compressed_prompt['compressed_prompt']+f" # 질문: {question}")

question = "친구탭의 1단계 효율은 얼마고 그 이유는 뭐야?"

compressed_prompt = llm_lingua.compress_prompt(prompt,
                                               instruction="",
                                               question=question,
                                               target_token=200)

print(compressed_prompt)

compressed_prompt = llm_lingua.compress_prompt(prompt,
                                               instruction="",
                                               question=question,
                                               target_token=300)

print(compressed_prompt['compressed_prompt'])

get_completion(prompt=prompt+f" # 질문: {question}")

get_completion(prompt=compressed_prompt['compressed_prompt']+f" # 질문: {question}")

